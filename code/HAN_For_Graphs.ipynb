{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Attention Network\n",
    "## Predict continuous values associated with graphs\n",
    "\n",
    "### Vincent Jacob, Cyril Equilbec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Nadam, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = os.path.dirname(os.path.abspath('').replace('\\\\', '/'))\n",
    "path_to_data = path_root + '/data/'\n",
    "path_to_code = path_root + '/code/'\n",
    "path_to_experiments = path_to_code + '/experiments/'\n",
    "sys.path.insert(0, path_to_code)\n",
    "for tgt in range(4):\n",
    "    sys.path.insert(0, path_to_experiments + 'target_' + str(tgt))\n",
    "    \n",
    "from make_model_tgt0 import make_model_tgt0\n",
    "from make_model_tgt1 import make_model_tgt1\n",
    "from make_model_tgt2 import make_model_tgt2\n",
    "from make_model_tgt3 import make_model_tgt3\n",
    "\n",
    "\n",
    "models = [make_model_tgt0, make_model_tgt1, make_model_tgt2, make_model_tgt3]\n",
    "is_GPU = True\n",
    "save_hist = False\n",
    "save_weights = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those two cells will create 3 files which are documents.npy (baseline pseudo-documents) and documents_i.npy (optimal pseudo-documents generated for target i={0, 3})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before running the notebook !\n",
    "# it will compute the pseudo-documents using the baseline procedure\n",
    "# we found that this method yielded corrects results for 2 targets\n",
    "\n",
    "# !python ./baseline/preprocessing_baseline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pseudo-docs adapted to target 0...\n",
      "documents generated for target 0\n",
      "document array shape: (93719, 160, 11)\n",
      "documents saved for target 0\n",
      "everything done in 835.89s \n",
      "\n",
      "Creating pseudo-docs adapted to target 3...\n",
      "documents generated for target 3\n",
      "document array shape: (93719, 100, 13)\n",
      "documents saved for target 3\n",
      "everything done in 350.05s \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# it will compute the pseudo-documents using our custom preprocessing method based on node2vec algorithms\n",
    "!python ./experiments/preprocess_inputs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TO TRAIN ONE MODEL FOR A GIVEN TARGET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precomputed documents (baseline)\n",
    "docs = np.load(path_to_data + 'documents.npy')\n",
    "# precomputed embbeding matrix (baseline)\n",
    "embeddings = np.load(path_to_data + 'embeddings.npy')\n",
    "\n",
    "###############################\n",
    "# Target we want to train\n",
    "tgt = 0\n",
    "###############################\n",
    "\n",
    "\n",
    "# Load training set and split it into train - validation sets\n",
    "with open(path_to_data + 'train_idxs.txt', 'r') as file:\n",
    "    train_idxs = file.read().splitlines()\n",
    "    \n",
    "train_idxs = [int(elt) for elt in train_idxs]\n",
    "\n",
    "idxs_select_train = np.random.choice(range(len(train_idxs)), size=int(len(train_idxs) * 0.80), replace=False)\n",
    "idxs_select_val = np.setdiff1d(range(len(train_idxs)), idxs_select_train)\n",
    "\n",
    "train_idxs_new = [train_idxs[elt] for elt in idxs_select_train]\n",
    "val_idxs = [train_idxs[elt] for elt in idxs_select_val]\n",
    "\n",
    "\n",
    "docs = np.load(path_to_data + 'documents.npy')\n",
    "docs_0 = np.load(path_to_data + 'documents_0.npy')\n",
    "docs_3 = np.load(path_to_data + 'documents_3.npy')\n",
    "dict_docs = {0 : docs_0, 1 : docs, 2 : docs, 3 : docs_3}\n",
    "\n",
    "data = dict_docs[tgt]\n",
    "\n",
    "docs_train = data[train_idxs_new, :, :]\n",
    "docs_val = data[val_idxs, :, :]\n",
    "\n",
    "# Load training and validation targets\n",
    "with open(path_to_data + 'targets/train/target_' + str(tgt) + '.txt', 'r') as file:\n",
    "    target = file.read().splitlines()\n",
    "    \n",
    "target_train = np.array([target[elt] for elt in idxs_select_train]).astype('float')\n",
    "target_val = np.array([target[elt] for elt in idxs_select_val]).astype('float')\n",
    "\n",
    "\n",
    "# Load test set\n",
    "with open(path_to_data + 'test_idxs.txt', 'r') as file:\n",
    "    test_idxs = file.read().splitlines()\n",
    "\n",
    "test_idxs = [int(elt) for elt in test_idxs]\n",
    "docs_test = docs[test_idxs, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of GRU units\n",
    "n_units = 60\n",
    "# mode by which outputs of the forward and backward RNNs will be combined.\n",
    "merge_mode = 'concat'\n",
    "drop_rate = 0.1\n",
    "# droupout rate after embedding layer\n",
    "drop_rate_emb = None\n",
    "# whether to use cosine sim or not (unormalized dot product)\n",
    "att_cosine = False\n",
    "# whether to use a MLP for computing hidden attention state\n",
    "use_dense_layer = True\n",
    "# the activation function used by the MLP \n",
    "att_activation = 'tanh'\n",
    "batch_size = 96\n",
    "nb_epochs = 10\n",
    "my_optimizer = Nadam()\n",
    "my_patience = 6\n",
    "\n",
    "###############################\n",
    "# Training from scratch or not\n",
    "pretrained = False\n",
    "###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the training if the loss doesn't improve up to 'my_patience' epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                   patience=my_patience,\n",
    "                                   mode='min')\n",
    "\n",
    "# save model corresponding to best epoch\n",
    "checkpointer = ModelCheckpoint(filepath=path_to_data + 'model_' + str(tgt), \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True)\n",
    "\n",
    "# reduce learning rate by 20% on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 160, 11)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 160, 60)           21980055  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 160, 120)          43920     \n",
      "_________________________________________________________________\n",
      "attention_with_context_2 (At (None, 120)               14640     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 22,045,936\n",
      "Trainable params: 129,301\n",
      "Non-trainable params: 21,916,635\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model compiled\n",
      "Train on 59980 samples, validate on 14995 samples\n",
      "Epoch 1/10\n",
      "59980/59980 [==============================] - 70s 1ms/step - loss: 0.6683 - mean_squared_error: 0.6683 - val_loss: 0.4087 - val_mean_squared_error: 0.4087\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.40873, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "Epoch 2/10\n",
      "59980/59980 [==============================] - 68s 1ms/step - loss: 0.3454 - mean_squared_error: 0.3454 - val_loss: 0.2862 - val_mean_squared_error: 0.2862\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.40873 to 0.28619, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "Epoch 3/10\n",
      "59980/59980 [==============================] - 69s 1ms/step - loss: 0.2743 - mean_squared_error: 0.2743 - val_loss: 0.3253 - val_mean_squared_error: 0.3253\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.28619\n",
      "Epoch 4/10\n",
      "59980/59980 [==============================] - 69s 1ms/step - loss: 0.2416 - mean_squared_error: 0.2416 - val_loss: 0.2257 - val_mean_squared_error: 0.2257\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.28619 to 0.22570, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "Epoch 5/10\n",
      "59980/59980 [==============================] - 70s 1ms/step - loss: 0.2219 - mean_squared_error: 0.2219 - val_loss: 0.2142 - val_mean_squared_error: 0.2142\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.22570 to 0.21424, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "Epoch 6/10\n",
      "59980/59980 [==============================] - 71s 1ms/step - loss: 0.2054 - mean_squared_error: 0.2054 - val_loss: 0.1899 - val_mean_squared_error: 0.1899\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.21424 to 0.18990, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "Epoch 7/10\n",
      "59980/59980 [==============================] - 72s 1ms/step - loss: 0.1895 - mean_squared_error: 0.1895 - val_loss: 0.2075 - val_mean_squared_error: 0.2075\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.18990\n",
      "Epoch 8/10\n",
      "59980/59980 [==============================] - 72s 1ms/step - loss: 0.1792 - mean_squared_error: 0.1792 - val_loss: 0.1722 - val_mean_squared_error: 0.1722\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.18990 to 0.17218, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "Epoch 9/10\n",
      "59980/59980 [==============================] - 71s 1ms/step - loss: 0.1684 - mean_squared_error: 0.1684 - val_loss: 0.1638 - val_mean_squared_error: 0.1638\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.17218 to 0.16384, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "Epoch 10/10\n",
      "59980/59980 [==============================] - 71s 1ms/step - loss: 0.1595 - mean_squared_error: 0.1595 - val_loss: 0.1531 - val_mean_squared_error: 0.1531\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.16384 to 0.15307, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "* * * * * * * target 0 done * * * * * * *\n"
     ]
    }
   ],
   "source": [
    "# Load the correct architecture according to target\n",
    "make_model = models[tgt]\n",
    "\n",
    "# Build a model according to the above hyperparameters\n",
    "model = make_model(n_units, merge_mode, drop_rate, drop_rate_emb, \n",
    "                   att_cosine, att_activation, use_dense_layer, \n",
    "                   embeddings, docs_train, is_GPU)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "if pretrained:\n",
    "    model.load_weights(path_to_data + 'model_' + str(tgt))\n",
    "    print(\"Weights loaded\")\n",
    "    \n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "                  optimizer=my_optimizer,\n",
    "                  metrics=['mse'])\n",
    "print(\"Model compiled\")\n",
    "\n",
    "# = = = = = training = = = = =\n",
    "\n",
    "if save_weights:\n",
    "    my_callbacks = [checkpointer, reduce_lr, early_stopping]\n",
    "else:\n",
    "    my_callbacks = [reduce_lr, early_stopping]\n",
    "    \n",
    "    \n",
    "\n",
    "model.fit(docs_train, \n",
    "              target_train,\n",
    "              batch_size = batch_size,\n",
    "              epochs = nb_epochs,\n",
    "              validation_data = (docs_val, target_val),\n",
    "              callbacks = my_callbacks)\n",
    "    \n",
    "\n",
    "    \n",
    "if save_hist:\n",
    "    hist = model.history.history\n",
    "    with open(path_to_data + 'model_history_' + str(tgt) + '.json', 'w') as file:\n",
    "        json.dump(hist, file, sort_keys=False, indent=4)\n",
    "\n",
    "print('* * * * * * * target', tgt, 'done * * * * * * *')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TO PREDICT ONE TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_han = []\n",
    "\n",
    "indx_tg0 = [i for i in range(0, 18744)]\n",
    "indx_tg1 = [i for i in range(18744, 37488)]\n",
    "indx_tg2 = [i for i in range(37488, 56232)]\n",
    "indx_tg3 = [i for i in range(56232,74976)]\n",
    "\n",
    "idx = [indx_tg0, indx_tg1, indx_tg2, indx_tg3]\n",
    "\n",
    "    \n",
    "# prediction mode\n",
    "drop_rate = 0.\n",
    "drop_rate_emb = 0.\n",
    "make_model = models[tgt]\n",
    "model =  make_model(n_units, merge_mode, drop_rate, drop_rate_emb, \n",
    "                   att_cosine, att_activation, use_dense_layer, \n",
    "                   embeddings, docs_test, is_GPU)\n",
    "    \n",
    "model.load_weights(path_to_data + 'model_' + str(tgt))\n",
    "all_preds_han.append(model.predict(docs_test).tolist())\n",
    "\n",
    "# flatten\n",
    "all_preds_han = [elt[0] for sublist in all_preds_han for elt in sublist]\n",
    "\n",
    "# write the predictions of a single target with the corrects indx\n",
    "with open(path_to_data + 'predictions_han_' + str(tgt) + '.txt', 'w') as file:\n",
    "    if tgt == 0:\n",
    "        file.write('id,pred\\n')\n",
    "    for id, pred in zip(idx[tgt], all_preds_han):\n",
    "        pred = format(pred, '.7f')\n",
    "        file.write(str(id) + ',' + pred + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING ALL THE MODELS\n",
    "## Each architecture should be in the correct folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "model compiled\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.39005, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.39005 to 0.32004, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.32004 to 0.27121, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.27121 to 0.23333, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.23333\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.23333 to 0.22883, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.22883 to 0.20769, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.20769 to 0.19708, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.19708 to 0.18846, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_0\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.18846\n",
      "* * * * * * * target 0 done * * * * * * *\n",
      "data loaded\n",
      "model compiled\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16268, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_1\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16268 to 0.13417, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_1\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13417 to 0.12554, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_1\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.12554\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.12554 to 0.11585, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_1\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.11585 to 0.10695, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_1\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.10695\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.10695\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.10695 to 0.10270, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_1\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.10270\n",
      "* * * * * * * target 1 done * * * * * * *\n",
      "data loaded\n",
      "model compiled\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.82876, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.82876\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.82876\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.82876 to 0.53636, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.53636\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.53636\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.53636\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.53636 to 0.43816, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.43816 to 0.42045, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.42045\n",
      "* * * * * * * target 2 done * * * * * * *\n",
      "data loaded\n",
      "model compiled\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11565, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_3\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11565 to 0.04553, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_3\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.04553\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.04553 to 0.04446, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_3\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.04446\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04446 to 0.03197, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_3\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.03197\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.03197\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03197 to 0.03005, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_3\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.03005\n",
      "* * * * * * * target 3 done * * * * * * *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!python ./experiments/training_experiments.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(\"read_results.py\").read())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict ALL TARGETS\n",
    "## This will create a csv file with the proper Kaggle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./experiments/predict_experiments.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate predictions of all targets\n",
    "### You can use the following code to generate a proper csv file from 4 txt files containing individual predictions for each targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def from_txt_to_csv(file_name, folder_name):\n",
    "#     \"\"\"\n",
    "#     Transform the output of the read_results_predict.py in a proper Kaggle Submission, i.e : a well formated csv file\n",
    "    \n",
    "#     inputs : \n",
    "#     - file_name is the name (string) of the txt file generated by the read_results_predict.py (without the .txt extension)\n",
    "#     - folder_name is the desired or existing name (string) of the folder where the submission will be stored into \n",
    "    \n",
    "#     output : None\n",
    "    \n",
    "#     \"\"\"\n",
    "#     path_root = os.path.dirname(os.path.abspath('').replace('\\\\', '/')) + \"/data/\"\n",
    "#     path_file = path_root  + file_name + \".txt\"\n",
    "#     path_folder = path_root + folder_name\n",
    "#     # create a directory if it doesn't exist yet\n",
    "#     if not os.path.exists(path_folder):\n",
    "#         os.makedirs(path_folder)\n",
    "      \n",
    "#     df = pd.read_csv(path_file)\n",
    "#     df.to_csv(path_folder + \"/\" + file_name + \".csv\", index=False)\n",
    "#     print(\"Submission saved in '{}'\".format(path_folder))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of txt files containing predictions for each target\n",
    "# filenames = [path_to_data + 'predictions_han_' + str(tgt) + '.txt' for tgt in range(4)]\n",
    "# with open(path_to_data + 'predictions_all.txt', 'w') as outfile:\n",
    "#     for fname in filenames:\n",
    "#         with open(fname) as infile:\n",
    "#             for line in infile:\n",
    "#                 outfile.write(line)\n",
    "\n",
    "# from_txt_to_csv('predictions_all', '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
