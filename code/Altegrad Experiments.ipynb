{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Altegrad Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TO TRAIN ONE TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 70, 11)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 70, 60)            21972855  \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 70, 60)            43920     \n",
      "_________________________________________________________________\n",
      "attention_with_context_4 (At (None, 60)                3720      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 22,024,216\n",
      "Trainable params: 107,581\n",
      "Non-trainable params: 21,916,635\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 59980 samples, validate on 14995 samples\n",
      "Epoch 1/60\n",
      "59980/59980 [==============================] - 30s 497us/step - loss: 0.7411 - mean_squared_error: 0.7411 - val_loss: 0.6481 - val_mean_squared_error: 0.6481\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64813, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 2/60\n",
      "59980/59980 [==============================] - 33s 555us/step - loss: 0.6001 - mean_squared_error: 0.6001 - val_loss: 0.6117 - val_mean_squared_error: 0.6117\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64813 to 0.61168, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 3/60\n",
      "59980/59980 [==============================] - 33s 550us/step - loss: 0.5804 - mean_squared_error: 0.5804 - val_loss: 0.5551 - val_mean_squared_error: 0.5551\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.61168 to 0.55508, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 4/60\n",
      "59980/59980 [==============================] - 30s 496us/step - loss: 0.5657 - mean_squared_error: 0.5657 - val_loss: 0.5601 - val_mean_squared_error: 0.5601\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.55508\n",
      "Epoch 5/60\n",
      "59980/59980 [==============================] - 30s 508us/step - loss: 0.5546 - mean_squared_error: 0.5546 - val_loss: 0.5315 - val_mean_squared_error: 0.5315\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.55508 to 0.53149, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 6/60\n",
      "59980/59980 [==============================] - 31s 521us/step - loss: 0.5468 - mean_squared_error: 0.5468 - val_loss: 0.5169 - val_mean_squared_error: 0.5169\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.53149 to 0.51694, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 7/60\n",
      "59980/59980 [==============================] - 31s 521us/step - loss: 0.5380 - mean_squared_error: 0.5380 - val_loss: 0.5366 - val_mean_squared_error: 0.5366\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.51694\n",
      "Epoch 8/60\n",
      "59980/59980 [==============================] - 32s 529us/step - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.5090 - val_mean_squared_error: 0.5090\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.51694 to 0.50902, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 9/60\n",
      "59980/59980 [==============================] - 31s 524us/step - loss: 0.5198 - mean_squared_error: 0.5198 - val_loss: 0.5041 - val_mean_squared_error: 0.5041\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.50902 to 0.50410, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 10/60\n",
      "59980/59980 [==============================] - 32s 531us/step - loss: 0.5136 - mean_squared_error: 0.5136 - val_loss: 0.4955 - val_mean_squared_error: 0.4955\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.50410 to 0.49546, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 11/60\n",
      "59980/59980 [==============================] - 34s 561us/step - loss: 0.5029 - mean_squared_error: 0.5029 - val_loss: 0.4862 - val_mean_squared_error: 0.4862\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.49546 to 0.48622, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 12/60\n",
      "59980/59980 [==============================] - 31s 522us/step - loss: 0.5012 - mean_squared_error: 0.5012 - val_loss: 0.5159 - val_mean_squared_error: 0.5159\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.48622\n",
      "Epoch 13/60\n",
      "44256/59980 [=====================>........] - ETA: 7s - loss: 0.4929 - mean_squared_error: 0.4929"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Input, Embedding, Dropout, Bidirectional, GRU, CuDNNGRU, TimeDistributed, Dense\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "tgt = 2\n",
    "\n",
    "is_GPU = True\n",
    "save_hist = False\n",
    "save_weights = True\n",
    "path_root = os.path.dirname(os.path.abspath('').replace('\\\\', '/'))\n",
    "path_to_data = path_root + '/data/'\n",
    "path_to_code = path_root + '/code/experiments/target_' + str(tgt) + '/'\n",
    "sys.path.insert(0, path_to_code)\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "from AttentionWithContext import AttentionWithContext\n",
    "from make_model_tgt2 import make_model\n",
    "\n",
    "# = = = = = hyper-parameters = = = = =\n",
    "\n",
    "sgd_opt = SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=False)\n",
    "\n",
    "n_units = 60\n",
    "drop_rate = 0.1\n",
    "batch_size = 96\n",
    "nb_epochs = 60\n",
    "my_optimizer = sgd_opt\n",
    "my_patience = 6\n",
    "\n",
    "# = = = = = data loading = = = = =\n",
    "\n",
    "docs = np.load(path_to_data + 'documents.npy')\n",
    "embeddings = np.load(path_to_data + 'embeddings.npy')\n",
    "\n",
    "with open(path_to_data + 'train_idxs.txt', 'r') as file:\n",
    "    train_idxs = file.read().splitlines()\n",
    "    \n",
    "train_idxs = [int(elt) for elt in train_idxs]\n",
    "\n",
    "idxs_select_train = np.random.choice(range(len(train_idxs)), size=int(len(train_idxs) * 0.80), replace=False)\n",
    "idxs_select_val = np.setdiff1d(range(len(train_idxs)), idxs_select_train)\n",
    "\n",
    "train_idxs_new = [train_idxs[elt] for elt in idxs_select_train]\n",
    "val_idxs = [train_idxs[elt] for elt in idxs_select_val]\n",
    "\n",
    "docs_train = docs[train_idxs_new, :, :]\n",
    "docs_val = docs[val_idxs, :, :]\n",
    "\n",
    "\n",
    "with open(path_to_data + 'targets/train/target_' + str(tgt) + '.txt', 'r') as file:\n",
    "    target = file.read().splitlines()\n",
    "    \n",
    "target_train = np.array([target[elt] for elt in idxs_select_train]).astype('float')\n",
    "target_val = np.array([target[elt] for elt in idxs_select_val]).astype('float')\n",
    "\n",
    "model = make_model(n_units, drop_rate, embeddings, docs_train, is_GPU)\n",
    "print(model.summary())\n",
    " \n",
    "model.compile(loss='mean_squared_error',\n",
    "                  optimizer=my_optimizer,\n",
    "                  metrics=['mse'])\n",
    "\n",
    "# = = = = = training = = = = =\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                   patience=my_patience,\n",
    "                                   mode='min')\n",
    "\n",
    "# save model corresponding to best epoch\n",
    "checkpointer = ModelCheckpoint(filepath=path_to_data + 'model_' + str(tgt), \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True)\n",
    "\n",
    "if save_weights:\n",
    "    my_callbacks = [early_stopping, checkpointer]\n",
    "else:\n",
    "    my_callbacks = [early_stopping]\n",
    "\n",
    "model.fit(docs_train, \n",
    "              target_train,\n",
    "              batch_size = batch_size,\n",
    "              epochs = nb_epochs,\n",
    "              validation_data = (docs_val, target_val),\n",
    "              callbacks = my_callbacks)\n",
    "    \n",
    "\n",
    "if save_hist:\n",
    "    hist = model.history.history\n",
    "    with open(path_to_data + 'model_history_' + str(tgt) + '.json', 'w') as file:\n",
    "        json.dump(hist, file, sort_keys=False, indent=4)\n",
    "\n",
    "print('* * * * * * * target', tgt, 'done * * * * * * *')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TO PREDICT ONE TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dropout, Bidirectional, GRU, CuDNNGRU, TimeDistributed, Dense\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "tgt = 1\n",
    "\n",
    "is_GPU = True\n",
    "save_hist = False\n",
    "path_root = os.path.dirname(os.path.abspath('').replace('\\\\', '/'))\n",
    "path_to_data = path_root + '/data/'\n",
    "path_to_code = path_root + '/code/experiments/target_' + str(tgt) + '/'\n",
    "sys.path.insert(0, path_to_code)\n",
    "\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "from AttentionWithContext import AttentionWithContext\n",
    "# to change \n",
    "from make_model_tgt1 import make_model\n",
    "# = = = = = = = = = = = = = = =\n",
    "\n",
    "docs = np.load(path_to_data + 'documents.npy')\n",
    "embeddings = np.load(path_to_data + 'embeddings.npy')\n",
    "\n",
    "with open(path_to_data + 'test_idxs.txt', 'r') as file:\n",
    "    test_idxs = file.read().splitlines()\n",
    "\n",
    "\n",
    "test_idxs = [int(elt) for elt in test_idxs]\n",
    "docs_test = docs[test_idxs,:,:]\n",
    "\n",
    "all_preds_han = []\n",
    "\n",
    "indx_tg0 = [i for i in range(0, 18744)]\n",
    "indx_tg1 = [i for i in range(18744, 37488)]\n",
    "indx_tg2 = [i for i in range(37488, 56232)]\n",
    "indx_tg3 = [i for i in range(56232,74976)]\n",
    "\n",
    "idx = [indx_tg0, indx_tg1, indx_tg2, indx_tg3]\n",
    "\n",
    "# * * * HAN * * * \n",
    "    \n",
    "# relevant hyper-parameters\n",
    "n_units = 60\n",
    "drop_rate = 0 # prediction mode\n",
    " \n",
    "model = make_model(n_units, drop_rate, embeddings, docs_test, is_GPU)\n",
    "    \n",
    "model.load_weights(path_to_data + 'model_' + str(tgt))\n",
    "all_preds_han.append(model.predict(docs_test).tolist())\n",
    "\n",
    "# flatten\n",
    "all_preds_han = [elt[0] for sublist in all_preds_han for elt in sublist]\n",
    "\n",
    "# write the predictions of a single target with the corrects indx\n",
    "with open(path_to_data + 'predictions_han_' + str(tgt) + '.txt', 'w') as file:\n",
    "    for idx, pred in zip(idx[tgt], all_preds_han):\n",
    "        if tgt==0:\n",
    "            file.write('id,pred\\n')\n",
    "        pred = format(pred, '.7f')\n",
    "        file.write(str(idx) + ',' + pred + '\\n')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate predictions of all targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def from_txt_to_csv(file_name, folder_name):\n",
    "    \"\"\"\n",
    "    Transform the output of the read_results_predict.py in a proper Kaggle Submission, i.e : a well formated csv file\n",
    "    \n",
    "    inputs : \n",
    "    - file_name is the name (string) of the txt file generated by the read_results_predict.py (without the .txt extension)\n",
    "    - folder_name is the desired or existing name (string) of the folder where the submission will be stored into \n",
    "    \n",
    "    output : None\n",
    "    \n",
    "    \"\"\"\n",
    "    # change me !\n",
    "    path_root = os.path.dirname(os.path.abspath('').replace('\\\\', '/')) + \"/data/\"\n",
    "    path_file = path_root  + file_name + \".txt\"\n",
    "    path_folder = path_root + folder_name\n",
    "    # create a directory if it doesn't exist yet\n",
    "    if not os.path.exists(path_folder):\n",
    "        os.makedirs(path_folder)\n",
    "      \n",
    "    df = pd.read_csv(path_file)\n",
    "    df.to_csv(path_folder + \"/\" + file_name + \".csv\", index=False)\n",
    "    print(\"Submission saved in '{}'\".format(path_folder))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved in 'D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/'\n"
     ]
    }
   ],
   "source": [
    "# list of txt files containing predictions for each target\n",
    "filenames = [path_to_data + 'predictions_han_' + str(tgt) + '.txt' for tgt in range(4)]\n",
    "with open(path_to_data + 'predictions_all.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "\n",
    "from_txt_to_csv('predictions_all', '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
