{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Altegrad Experiments</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Nadam, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input, Embedding, Dropout, Bidirectional, GRU, CuDNNGRU, TimeDistributed, Dense\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target we want to train\n",
    "tgt = 2\n",
    "is_GPU = True\n",
    "save_hist = False\n",
    "save_weights = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = os.path.dirname(os.path.abspath('').replace('\\\\', '/'))\n",
    "path_to_data = path_root + '/data/'\n",
    "path_to_code = path_root + '/code/experiments/target_' + str(tgt) + '/'\n",
    "sys.path.insert(0, path_to_code)\n",
    "\n",
    "# precomputed documents\n",
    "docs = np.load(path_to_data + 'documents.npy')\n",
    "# precomputed embbeding matrix\n",
    "embeddings = np.load(path_to_data + 'embeddings.npy')\n",
    "\n",
    "\n",
    "# Load training set and split it into train - validation sets\n",
    "with open(path_to_data + 'train_idxs.txt', 'r') as file:\n",
    "    train_idxs = file.read().splitlines()\n",
    "    \n",
    "train_idxs = [int(elt) for elt in train_idxs]\n",
    "\n",
    "idxs_select_train = np.random.choice(range(len(train_idxs)), size=int(len(train_idxs) * 0.90), replace=False)\n",
    "idxs_select_val = np.setdiff1d(range(len(train_idxs)), idxs_select_train)\n",
    "\n",
    "train_idxs_new = [train_idxs[elt] for elt in idxs_select_train]\n",
    "val_idxs = [train_idxs[elt] for elt in idxs_select_val]\n",
    "\n",
    "docs_train = docs[train_idxs_new, :, :]\n",
    "docs_val = docs[val_idxs, :, :]\n",
    "\n",
    "# Load traing and validation targets\n",
    "with open(path_to_data + 'targets/train/target_' + str(tgt) + '.txt', 'r') as file:\n",
    "    target = file.read().splitlines()\n",
    "    \n",
    "target_train = np.array([target[elt] for elt in idxs_select_train]).astype('float')\n",
    "target_val = np.array([target[elt] for elt in idxs_select_val]).astype('float')\n",
    "\n",
    "\n",
    "# Load test set\n",
    "with open(path_to_data + 'test_idxs.txt', 'r') as file:\n",
    "    test_idxs = file.read().splitlines()\n",
    "\n",
    "test_idxs = [int(elt) for elt in test_idxs]\n",
    "docs_test = docs[test_idxs,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TO TRAIN ONE TARGET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_units = 45\n",
    "drop_rate = 0.5 \n",
    "drop_rate_emb = 0.1\n",
    "att_cosine = False\n",
    "att_activation = 'sigmoid'\n",
    "batch_size = 120\n",
    "nb_epochs = 120\n",
    "my_optimizer = Nadam()\n",
    "my_patience = 12\n",
    "\n",
    "#######################\n",
    "# Training from scratch or not\n",
    "pretrained = False\n",
    "#######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the training if the loss doesn't improve up to 'my_patience' epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                   patience=my_patience,\n",
    "                                   mode='min')\n",
    "\n",
    "# save model corresponding to best epoch\n",
    "checkpointer = ModelCheckpoint(filepath=path_to_data + 'model_' + str(tgt), \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True)\n",
    "\n",
    "# reduce learning rate by 20% on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 70, 11)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 70, 45)            21935130  \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 70, 45)            24840     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 70, 45)            180       \n",
      "_________________________________________________________________\n",
      "attention_with_context_2 (At (None, 45)                2115      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 45)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 46        \n",
      "=================================================================\n",
      "Total params: 21,962,311\n",
      "Trainable params: 45,496\n",
      "Non-trainable params: 21,916,815\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 67477 samples, validate on 7498 samples\n",
      "Epoch 1/120\n",
      "67477/67477 [==============================] - 94s 1ms/step - loss: 0.6896 - mean_squared_error: 0.6896 - val_loss: 0.6180 - val_mean_squared_error: 0.6180\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61797, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 2/120\n",
      "67477/67477 [==============================] - 91s 1ms/step - loss: 0.5458 - mean_squared_error: 0.5458 - val_loss: 0.5710 - val_mean_squared_error: 0.5710\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61797 to 0.57101, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 3/120\n",
      "67477/67477 [==============================] - 91s 1ms/step - loss: 0.5155 - mean_squared_error: 0.5155 - val_loss: 0.5526 - val_mean_squared_error: 0.5526\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57101 to 0.55258, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 4/120\n",
      "67477/67477 [==============================] - 92s 1ms/step - loss: 0.5001 - mean_squared_error: 0.5001 - val_loss: 0.5478 - val_mean_squared_error: 0.5478\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.55258 to 0.54783, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 5/120\n",
      "67477/67477 [==============================] - 91s 1ms/step - loss: 0.4953 - mean_squared_error: 0.4953 - val_loss: 0.5675 - val_mean_squared_error: 0.5675\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54783\n",
      "Epoch 6/120\n",
      "67477/67477 [==============================] - 91s 1ms/step - loss: 0.4839 - mean_squared_error: 0.4839 - val_loss: 0.4698 - val_mean_squared_error: 0.4698\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.54783 to 0.46981, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 7/120\n",
      "67477/67477 [==============================] - 91s 1ms/step - loss: 0.4741 - mean_squared_error: 0.4741 - val_loss: 0.5005 - val_mean_squared_error: 0.5005\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.46981\n",
      "Epoch 8/120\n",
      "67477/67477 [==============================] - 92s 1ms/step - loss: 0.4668 - mean_squared_error: 0.4668 - val_loss: 0.4805 - val_mean_squared_error: 0.4805\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.46981\n",
      "Epoch 9/120\n",
      "67477/67477 [==============================] - 91s 1ms/step - loss: 0.4597 - mean_squared_error: 0.4597 - val_loss: 0.4982 - val_mean_squared_error: 0.4982\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.46981\n",
      "Epoch 10/120\n",
      "67477/67477 [==============================] - 91s 1ms/step - loss: 0.4569 - mean_squared_error: 0.4569 - val_loss: 0.4670 - val_mean_squared_error: 0.4670\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.46981 to 0.46699, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 11/120\n",
      "67477/67477 [==============================] - 91s 1ms/step - loss: 0.4470 - mean_squared_error: 0.4470 - val_loss: 0.4473 - val_mean_squared_error: 0.4473\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.46699 to 0.44730, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 12/120\n",
      "67477/67477 [==============================] - 91s 1ms/step - loss: 0.4449 - mean_squared_error: 0.4449 - val_loss: 0.4358 - val_mean_squared_error: 0.4358\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.44730 to 0.43580, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 13/120\n",
      "67477/67477 [==============================] - 91s 1ms/step - loss: 0.4383 - mean_squared_error: 0.4383 - val_loss: 0.4293 - val_mean_squared_error: 0.4293\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.43580 to 0.42928, saving model to D:/Scolaire/Code/Python/Machine_Learning/Kaggle_Challenges_M2/HAN_Graphs/data/model_2\n",
      "Epoch 14/120\n",
      "15000/67477 [=====>........................] - ETA: 1:08 - loss: 0.4388 - mean_squared_error: 0.4388"
     ]
    }
   ],
   "source": [
    "# Load the correct attention mechanism and architecture according to target\n",
    "from AttentionWithContext_tgt2 import AttentionWithContext\n",
    "from make_model_tgt2 import make_model\n",
    "\n",
    "# Build a model according to the above hyperparameters\n",
    "model = make_model(n_units, drop_rate, drop_rate_emb, att_cosine, att_activation, embeddings, docs_train, is_GPU)\n",
    "print(model.summary())\n",
    "\n",
    "if pretrained:\n",
    "    model.load_weights(path_to_data + 'model_' + str(tgt))\n",
    "    print(\"Weights loaded\")\n",
    "    \n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "                  optimizer=my_optimizer,\n",
    "                  metrics=['mse'])\n",
    "\n",
    "# = = = = = training = = = = =\n",
    "\n",
    "if save_weights:\n",
    "    my_callbacks = [checkpointer, reduce_lr, early_stopping]\n",
    "else:\n",
    "    my_callbacks = [reduce_lr, early_stopping]\n",
    "    \n",
    "    \n",
    "\n",
    "model.fit(docs_train, \n",
    "              target_train,\n",
    "              batch_size = batch_size,\n",
    "              epochs = nb_epochs,\n",
    "              validation_data = (docs_val, target_val),\n",
    "              callbacks = my_callbacks)\n",
    "    \n",
    "\n",
    "    \n",
    "if save_hist:\n",
    "    hist = model.history.history\n",
    "    with open(path_to_data + 'model_history_' + str(tgt) + '.json', 'w') as file:\n",
    "        json.dump(hist, file, sort_keys=False, indent=4)\n",
    "\n",
    "print('* * * * * * * target', tgt, 'done * * * * * * *')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best : 0.31090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TO PREDICT ONE TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_han = []\n",
    "\n",
    "indx_tg0 = [i for i in range(0, 18744)]\n",
    "indx_tg1 = [i for i in range(18744, 37488)]\n",
    "indx_tg2 = [i for i in range(37488, 56232)]\n",
    "indx_tg3 = [i for i in range(56232,74976)]\n",
    "\n",
    "idx = [indx_tg0, indx_tg1, indx_tg2, indx_tg3]\n",
    "\n",
    "# * * * HAN * * * \n",
    "    \n",
    "# relevant hyper-parameters\n",
    "n_units = 45\n",
    "drop_rate = 0 # prediction mode\n",
    " \n",
    "model = make_model(n_units, drop_rate, embeddings, docs_test, is_GPU)\n",
    "    \n",
    "model.load_weights(path_to_data + 'model_' + str(tgt))\n",
    "all_preds_han.append(model.predict(docs_test).tolist())\n",
    "\n",
    "# flatten\n",
    "all_preds_han = [elt[0] for sublist in all_preds_han for elt in sublist]\n",
    "\n",
    "# write the predictions of a single target with the corrects indx\n",
    "with open(path_to_data + 'predictions_han_' + str(tgt) + '.txt', 'w') as file:\n",
    "    if tgt==0:\n",
    "        file.write('id,pred\\n')\n",
    "    for idx, pred in zip(idx[tgt], all_preds_han):\n",
    "        pred = format(pred, '.7f')\n",
    "        file.write(str(idx) + ',' + pred + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate predictions of all targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_txt_to_csv(file_name, folder_name):\n",
    "    \"\"\"\n",
    "    Transform the output of the read_results_predict.py in a proper Kaggle Submission, i.e : a well formated csv file\n",
    "    \n",
    "    inputs : \n",
    "    - file_name is the name (string) of the txt file generated by the read_results_predict.py (without the .txt extension)\n",
    "    - folder_name is the desired or existing name (string) of the folder where the submission will be stored into \n",
    "    \n",
    "    output : None\n",
    "    \n",
    "    \"\"\"\n",
    "    path_root = os.path.dirname(os.path.abspath('').replace('\\\\', '/')) + \"/data/\"\n",
    "    path_file = path_root  + file_name + \".txt\"\n",
    "    path_folder = path_root + folder_name\n",
    "    # create a directory if it doesn't exist yet\n",
    "    if not os.path.exists(path_folder):\n",
    "        os.makedirs(path_folder)\n",
    "      \n",
    "    df = pd.read_csv(path_file)\n",
    "    df.to_csv(path_folder + \"/\" + file_name + \".csv\", index=False)\n",
    "    print(\"Submission saved in '{}'\".format(path_folder))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of txt files containing predictions for each target\n",
    "filenames = [path_to_data + 'predictions_han_' + str(tgt) + '.txt' for tgt in range(4)]\n",
    "with open(path_to_data + 'predictions_all.txt', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "\n",
    "from_txt_to_csv('predictions_all', '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
