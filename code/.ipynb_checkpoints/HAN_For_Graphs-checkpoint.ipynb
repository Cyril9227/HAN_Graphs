{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Attention Network\n",
    "## Predict continuous values associated with graphs\n",
    "\n",
    "### Vincent Jacob, Cyril Equilbec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Nadam, Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = os.path.dirname(os.path.abspath('').replace('\\\\', '/'))\n",
    "path_to_data = path_root + '/data/'\n",
    "path_to_code = path_root + '/code/'\n",
    "path_to_experiments = path_to_code + '/experiments/'\n",
    "sys.path.insert(0, path_to_code)\n",
    "for tgt in range(4):\n",
    "    sys.path.insert(0, path_to_experiments + 'target_' + str(tgt))\n",
    "    \n",
    "from make_model_tgt0 import make_model_tgt0\n",
    "from make_model_tgt1 import make_model_tgt1\n",
    "from make_model_tgt2 import make_model_tgt2\n",
    "from make_model_tgt3 import make_model_tgt3\n",
    "\n",
    "\n",
    "models = [make_model_tgt0, make_model_tgt1, make_model_tgt2, make_model_tgt3]\n",
    "is_GPU = True\n",
    "save_hist = False\n",
    "save_weights = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those two cells will create 3 files which are documents.npy (baseline pseudo-documents) and documents_i.npy (optimal pseudo-documents generated for target i={0, 3})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before running the notebook !\n",
    "# it will compute the pseudo-documents using the baseline procedure\n",
    "# we found that this method yielded corrects results for 2 targets\n",
    "\n",
    "# !python ./baseline/preprocessing_baseline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will compute the pseudo-documents using our custom preprocessing method based on node2vec algorithms\n",
    "!python ./experiments/preprocess_inputs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TO TRAIN ONE MODEL FOR A GIVEN TARGET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precomputed documents (baseline)\n",
    "docs = np.load(path_to_data + 'documents.npy')\n",
    "# precomputed embbeding matrix (baseline)\n",
    "embeddings = np.load(path_to_data + 'embeddings.npy')\n",
    "\n",
    "###############################\n",
    "# Target we want to train\n",
    "tgt = 0\n",
    "###############################\n",
    "\n",
    "\n",
    "# Load training set and split it into train - validation sets\n",
    "with open(path_to_data + 'train_idxs.txt', 'r') as file:\n",
    "    train_idxs = file.read().splitlines()\n",
    "    \n",
    "train_idxs = [int(elt) for elt in train_idxs]\n",
    "\n",
    "idxs_select_train = np.random.choice(range(len(train_idxs)), size=int(len(train_idxs) * 0.80), replace=False)\n",
    "idxs_select_val = np.setdiff1d(range(len(train_idxs)), idxs_select_train)\n",
    "\n",
    "train_idxs_new = [train_idxs[elt] for elt in idxs_select_train]\n",
    "val_idxs = [train_idxs[elt] for elt in idxs_select_val]\n",
    "\n",
    "\n",
    "docs = np.load(path_to_data + 'documents.npy')\n",
    "docs_0 = np.load(path_to_data + 'documents_0.npy')\n",
    "docs_3 = np.load(path_to_data + 'documents_3.npy')\n",
    "dict_docs = {0 : docs_0, 1 : docs, 2 : docs, 3 : docs_3}\n",
    "\n",
    "data = dict_docs[tgt]\n",
    "\n",
    "docs_train = data[train_idxs_new, :, :]\n",
    "docs_val = data[val_idxs, :, :]\n",
    "\n",
    "# Load training and validation targets\n",
    "with open(path_to_data + 'targets/train/target_' + str(tgt) + '.txt', 'r') as file:\n",
    "    target = file.read().splitlines()\n",
    "    \n",
    "target_train = np.array([target[elt] for elt in idxs_select_train]).astype('float')\n",
    "target_val = np.array([target[elt] for elt in idxs_select_val]).astype('float')\n",
    "\n",
    "\n",
    "# Load test set\n",
    "with open(path_to_data + 'test_idxs.txt', 'r') as file:\n",
    "    test_idxs = file.read().splitlines()\n",
    "\n",
    "test_idxs = [int(elt) for elt in test_idxs]\n",
    "docs_test = docs[test_idxs, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of GRU units\n",
    "n_units = 60\n",
    "# mode by which outputs of the forward and backward RNNs will be combined.\n",
    "merge_mode = 'concat'\n",
    "drop_rate = 0.1\n",
    "# droupout rate after embedding layer\n",
    "drop_rate_emb = None\n",
    "# whether to use cosine sim or not (unormalized dot product)\n",
    "att_cosine = False\n",
    "# whether to use a MLP for computing hidden attention state\n",
    "use_dense_layer = True\n",
    "# the activation function used by the MLP \n",
    "att_activation = 'tanh'\n",
    "batch_size = 96\n",
    "nb_epochs = 10\n",
    "my_optimizer = Nadam()\n",
    "my_patience = 6\n",
    "\n",
    "###############################\n",
    "# Training from scratch or not\n",
    "pretrained = False\n",
    "###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the training if the loss doesn't improve up to 'my_patience' epochs\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                   patience=my_patience,\n",
    "                                   mode='min')\n",
    "\n",
    "# save model corresponding to best epoch\n",
    "checkpointer = ModelCheckpoint(filepath=path_to_data + 'model_' + str(tgt), \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True)\n",
    "\n",
    "# reduce learning rate by 20% on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the correct architecture according to target\n",
    "make_model = models[tgt]\n",
    "\n",
    "# Build a model according to the above hyperparameters\n",
    "model = make_model(n_units, merge_mode, drop_rate, drop_rate_emb, \n",
    "                   att_cosine, att_activation, use_dense_layer, \n",
    "                   embeddings, docs_train, is_GPU)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "if pretrained:\n",
    "    model.load_weights(path_to_data + 'model_' + str(tgt))\n",
    "    print(\"Weights loaded\")\n",
    "    \n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "                  optimizer=my_optimizer,\n",
    "                  metrics=['mse'])\n",
    "print(\"Model compiled\")\n",
    "\n",
    "# = = = = = training = = = = =\n",
    "\n",
    "if save_weights:\n",
    "    my_callbacks = [checkpointer, reduce_lr, early_stopping]\n",
    "else:\n",
    "    my_callbacks = [reduce_lr, early_stopping]\n",
    "    \n",
    "    \n",
    "\n",
    "model.fit(docs_train, \n",
    "              target_train,\n",
    "              batch_size = batch_size,\n",
    "              epochs = nb_epochs,\n",
    "              validation_data = (docs_val, target_val),\n",
    "              callbacks = my_callbacks)\n",
    "    \n",
    "\n",
    "    \n",
    "if save_hist:\n",
    "    hist = model.history.history\n",
    "    with open(path_to_data + 'model_history_' + str(tgt) + '.json', 'w') as file:\n",
    "        json.dump(hist, file, sort_keys=False, indent=4)\n",
    "\n",
    "print('* * * * * * * target', tgt, 'done * * * * * * *')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TO PREDICT ONE TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_han = []\n",
    "\n",
    "indx_tg0 = [i for i in range(0, 18744)]\n",
    "indx_tg1 = [i for i in range(18744, 37488)]\n",
    "indx_tg2 = [i for i in range(37488, 56232)]\n",
    "indx_tg3 = [i for i in range(56232,74976)]\n",
    "\n",
    "idx = [indx_tg0, indx_tg1, indx_tg2, indx_tg3]\n",
    "\n",
    "    \n",
    "# prediction mode\n",
    "drop_rate = 0.\n",
    "drop_rate_emb = 0.\n",
    "make_model = models[tgt]\n",
    "model =  make_model(n_units, merge_mode, drop_rate, drop_rate_emb, \n",
    "                   att_cosine, att_activation, use_dense_layer, \n",
    "                   embeddings, docs_test, is_GPU)\n",
    "    \n",
    "model.load_weights(path_to_data + 'model_' + str(tgt))\n",
    "all_preds_han.append(model.predict(docs_test).tolist())\n",
    "\n",
    "# flatten\n",
    "all_preds_han = [elt[0] for sublist in all_preds_han for elt in sublist]\n",
    "\n",
    "# write the predictions of a single target with the corrects indx\n",
    "with open(path_to_data + 'predictions_han_' + str(tgt) + '.txt', 'w') as file:\n",
    "    if tgt == 0:\n",
    "        file.write('id,pred\\n')\n",
    "    for idx, pred in zip(idx[tgt], all_preds_han):\n",
    "        pred = format(pred, '.7f')\n",
    "        file.write(str(idx) + ',' + pred + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING ALL THE MODELS\n",
    "## Each architectures should be in the correct folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./experiments/training_experiments.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(\"read_results.py\").read())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict ALL TARGETS\n",
    "## This will create a csv file with the proper Kaggle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./experiments/predict_experiments.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate predictions of all targets\n",
    "### You can use the following code to generate a proper csv file from 4 txt files containing individual predictions for each targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def from_txt_to_csv(file_name, folder_name):\n",
    "#     \"\"\"\n",
    "#     Transform the output of the read_results_predict.py in a proper Kaggle Submission, i.e : a well formated csv file\n",
    "    \n",
    "#     inputs : \n",
    "#     - file_name is the name (string) of the txt file generated by the read_results_predict.py (without the .txt extension)\n",
    "#     - folder_name is the desired or existing name (string) of the folder where the submission will be stored into \n",
    "    \n",
    "#     output : None\n",
    "    \n",
    "#     \"\"\"\n",
    "#     path_root = os.path.dirname(os.path.abspath('').replace('\\\\', '/')) + \"/data/\"\n",
    "#     path_file = path_root  + file_name + \".txt\"\n",
    "#     path_folder = path_root + folder_name\n",
    "#     # create a directory if it doesn't exist yet\n",
    "#     if not os.path.exists(path_folder):\n",
    "#         os.makedirs(path_folder)\n",
    "      \n",
    "#     df = pd.read_csv(path_file)\n",
    "#     df.to_csv(path_folder + \"/\" + file_name + \".csv\", index=False)\n",
    "#     print(\"Submission saved in '{}'\".format(path_folder))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of txt files containing predictions for each target\n",
    "# filenames = [path_to_data + 'predictions_han_' + str(tgt) + '.txt' for tgt in range(4)]\n",
    "# with open(path_to_data + 'predictions_all.txt', 'w') as outfile:\n",
    "#     for fname in filenames:\n",
    "#         with open(fname) as infile:\n",
    "#             for line in infile:\n",
    "#                 outfile.write(line)\n",
    "\n",
    "# from_txt_to_csv('predictions_all', '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
